{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODoHpuqLG8Relt70YeCGZQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofzlo/NLP-tutorial/blob/main/2.%20Text%20Preprocessing/NLP_02_08_one_hot_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02-08 원-핫 인코딩(One-Hot Encoding)\n",
        "- 원-핫 인코딩(One-Hot Encoding) : 단어를 숫자로 표현하는 가장 기본적인 표현 방법   \n",
        "- 단어 집합을 먼저 만들어야 한다."
      ],
      "metadata": {
        "id": "s1TRGnKIuhD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 원-핫 인코딩(One-Hot Encoding)이란?\n",
        "- 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어 벡터 표현 방식   \n",
        "- 표현된 벡터를 원-핫 벡터(One-Hot vector)라고 한다.   \n",
        "\n",
        "### 　\n",
        "\n",
        "#### 원-핫 인코딩 과정\n",
        "- 1. 정수 인코딩을 수행한다   \n",
        "  -  각 단어에 고유한 정수를 부여한다.\n",
        "- 2. 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고, 해당 위치에 1을 부여하고 다른 단어의 인덱스 위치에는 0을 부여한다."
      ],
      "metadata": {
        "id": "gk8K7rEnuu5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tztb7Ttzua-I",
        "outputId": "8291b88c-25c1-4ab5-bc41-fd94f3bb4fd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
            "['나', '는', '자연어', '처리', '를', '배운다']\n"
          ]
        }
      ],
      "source": [
        "# Okt 형태소 분석기를 통해 문장에 대해 토큰화 수행\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt  \n",
        "\n",
        "okt = Okt()  \n",
        "tokens = okt.morphs(\"나는 자연어 처리를 배운다\")  \n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 토큰에 대해서 고유한 정수를 부여\n",
        "# 문장의 길이가 짧아 각 단어의 빈도수 순을 고려하지 않는다.\n",
        "\n",
        "word_to_index = {word : index for index, word in enumerate(tokens)}\n",
        "print('단어 집합 :',word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axpPiZWYvQ01",
        "outputId": "93f7f36a-0ab4-4608-bdf7-c7c60fe16abf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "  one_hot_vector = [0]*(len(word_to_index))\n",
        "  index = word_to_index[word]\n",
        "  one_hot_vector[index] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "i9Nh0nd-vo3E"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '자연어'라는 단어의 원-핫 벡터 얻기\n",
        "one_hot_encoding(\"자연어\", word_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OSkWjR-vrD0",
        "outputId": "748d7043-af00-4f4f-93c9-c6c27c07aac2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 케라스(Keras)를 이용한 원-핫 인코딩(One-Hot Encoding)"
      ],
      "metadata": {
        "id": "Rr2cGkl1v8rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
      ],
      "metadata": {
        "id": "jOhgeMi0vtEk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "print('단어 집합 :',tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXBijaD_wANM",
        "outputId": "5e711e26-c32c-4795-f101-e996b954e78a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위와 같이 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면, texts_to_sequences()를 통해서 이를 정수 시퀀스로 변환가능\n",
        "# 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 만들어 확인\n",
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "904I1iUOwBcD",
        "outputId": "3c811ffa-6a52-4091-fbcb-d9dafc83226e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 5, 1, 6, 3, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정수 인코딩 된 결과로부터 원-핫 인코딩을 수행하는 to_categorical()를 지원\n",
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4RSS4JRwHBb",
        "outputId": "8f16ec4b-9a39-4a1f-c6af-ec4989e7f6e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 원-핫 인코딩(One-Hot Encoding)의 한계\n",
        "- 단어의 개수가 늘어날수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점 = 벡터의 차원이 늘어난다.   \n",
        "  - 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 된다.   \n",
        "- 단어의 유사도를 표현하지 못한다.   \n",
        "  - 검색 시스템 등에서는 문제가 될 소지가 있음.\n",
        "- 이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법이 있다.\n",
        "  - 1. **`카운트 기반`**의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있다.   \n",
        "  - 2. **`예측 기반`**으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있다.   \n",
        "  - 3. 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 GloVe 방법이 있다."
      ],
      "metadata": {
        "id": "66mtxfvkwnB2"
      }
    }
  ]
}