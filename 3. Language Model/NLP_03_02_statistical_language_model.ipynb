{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmLxQfsY9pIpHIVoqSFp/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofzlo/NLP-tutorial/blob/main/3.%20Language%20Model/NLP_03_02_statistical_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03-02 통계적 언어 모델(Statistical Language Model, SLM)\n",
        "\n",
        "## 1. 조건부 확률\n",
        "- 연쇄 법칙(chain rule)   \n",
        "  - ![](https://user-images.githubusercontent.com/90624848/215724430-5064e938-c584-418f-bc70-e5bc5358697a.png)   "
      ],
      "metadata": {
        "id": "jluHTo9nKr_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 문장에 대한 확률\n",
        "- 조건부 확률을 쓰는 이유: 문맥이라는 관계로 인해 이전 단어의 영향을 받아 단어가 나오고, 모든 단어로부터 하나의 문장이 완성되기 때문이다.   \n",
        "- 조건부 확률의 관점에서 문장의 확률 표현   \n",
        "  - ![](https://user-images.githubusercontent.com/90624848/215725020-6723bd66-d7a5-42b3-9dd8-4c9e3036e551.png)   \n",
        "\n",
        "#### 　\n",
        "\n",
        "예시(An adorable little boy is spreading smiles)   \n",
        "![](https://user-images.githubusercontent.com/90624848/215725187-82778c06-ec0b-4a2c-9d4d-9c58c9da4d9b.png)"
      ],
      "metadata": {
        "id": "RbhBcLPzKr8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 카운트 기반의 접근\n",
        "- 카운트에 기반하여 이전 단어로부터 다음 단어에 대한 확률을 계산한다.   \n",
        "\n",
        "![image](https://user-images.githubusercontent.com/90624848/215725709-e76e619d-f1e3-42d9-a30b-2e8519252905.png)\n",
        "\n",
        "기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 하면, 이 경우 P(is | An adorable little boy)는 30%\n"
      ],
      "metadata": {
        "id": "9BEqWnGBKr6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
        "- 언어 모델은 실생활에서 사용되는 언어의 확률 분포를 근사 모델링한다.   \n",
        "  - 기계에서 많은 코퍼스를 훈련시켜 언어 모델을 통해 현실에서의 확률 분포를 근사하는 것이 모델의 목표   \n",
        "  - 카운트 기반으로 접근할 경우 갖고 있는 코퍼스(corpus)가 굉장히 많아야 한다.   \n",
        "- **`희소 문제(sparsity problem)`** : 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링 못하는 문제  \n",
        "  - 완화 방법 : n-gram, 스무딩, 백오프 등 여러가지 일반화 기법이 존재한다.   \n",
        "  - But, 희소 문제에 대한 근본적인 해결책은 되지 못함. => 언어 모델의 트렌드가 인공 신경망 언어 모델로 넘어가게 됨."
      ],
      "metadata": {
        "id": "FPS13iTQKr4F"
      }
    }
  ]
}