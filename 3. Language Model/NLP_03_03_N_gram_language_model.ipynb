{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnWCIhXvEjFfehl332hcYm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofzlo/NLP-tutorial/blob/main/3.%20Language%20Model/NLP_03_03_N_gram_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 03-03 N-gram 언어 모델(N-gram Language Model)\n",
        "- SLM의 일종 : 카운트에 기반한 통계적 접근을 사용한다.   \n",
        "- 이전에 등장한 모든 단어를 고려하지 않고, 일부 단어만 고려하는 접근 방법을 사용한다.   \n",
        "  - 일부 단어 몇 개를 보는지 결정하는데 이것이 n-gram에서 n이 가지는 의미"
      ],
      "metadata": {
        "id": "wgenN35WQP5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 코퍼스에서 카운트하지 못하는 경우의 감소\n",
        "- SLM의 한계\n",
        "  - 훈련 코퍼스 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다   \n",
        "  - 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다.   \n",
        "  - 카운트 할 수 없는 가능성이 높은 경우가 있다.\n",
        "- 참고하는 단어들을 줄이면 카운트를 할 수 있는 가능성을 높일 수 있다.   \n",
        "![](https://user-images.githubusercontent.com/90624848/215728485-bd4f4f8a-56e5-49a5-b039-8b8571ca8f13.png)  \n",
        "![](https://user-images.githubusercontent.com/90624848/215729260-bb554604-59f8-499f-ac33-4d6860046791.png)   \n",
        "- 단어의 확률을 구하고자 기존 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하는 것   "
      ],
      "metadata": {
        "id": "r4P-bKXnQP9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. N-gram   \n",
        "- 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주한다.   \n",
        "- 예시 : An adorable little boy is spreading smiles   \n",
        "  - (n = 1) unigrams : an, adorable, little, boy, is, spreading, smiles  \n",
        "  - (n = 2) bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles  \n",
        "  - (n = 3) trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles  \n",
        "  - (n = 4) 4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles  \n",
        "\n",
        "- n = 4라고 한 4-gram을 사용할 경우 앞의 3 단어만 고려한다.   \n",
        "![](https://wikidocs.net/images/page/21692/n-gram.PNG)   \n",
        "![](https://user-images.githubusercontent.com/90624848/215732184-77399488-81a2-484a-a090-32104edd3626.png)"
      ],
      "metadata": {
        "id": "xvidmhwuQP_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. N-gram Language Model의 한계\n",
        "- 앞의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다.   \n",
        "- 전체 문장을 고려한 언어 모델보다 정확도가 떨어질 수밖에 없다. \n",
        "\n",
        "### (1) 희소 문제(Sparsity Problem)   \n",
        "- 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높인 것일 뿐, 여전히 희소 문제가 존재한다.   \n",
        "\n",
        "### (2) n을 선택하는 것은 trade-off 문제\n",
        "- 임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있다.   \n",
        "- n을 크게 선택할 경우   \n",
        "  - 희소 문제가 심해진다.    \n",
        "  - 모델 사이즈가 커진다.   \n",
        "- n을 작게 선택할 경우\n",
        "  - 카운트는 잘 되지만 근사의 정확도는 현실의 확률분포와 멀어진다.   \n",
        "\n",
        "**=> 결론: n은 최대 5를 넘게 잡아서는 안 된다고 권장**   "
      ],
      "metadata": {
        "id": "ZnmhUOjrQQCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 적용 분야(Domain)에 맞는 코퍼스의 수집\n",
        "- 어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 당연히 다르다.   \n",
        "- 언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아진다."
      ],
      "metadata": {
        "id": "J3uOwzUWQQFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)\n",
        "- N-gram Language Model의 한계점을 극복하기위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 일반화(generalization) 방법들이 존재   \n",
        "- 그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못함.   \n",
        "- 대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 인공 신경망을 이용한 언어 모델이 많이 사용되고 있다."
      ],
      "metadata": {
        "id": "InxqKkxMQQIR"
      }
    }
  ]
}