{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3076vXxrS/eT0a6+GKu8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofzlo/NLP-tutorial/blob/main/8.%20RNN/NLP_08_05_RNN_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 08-05 RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)\n",
        "\n",
        "## 1. RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)\n",
        "- time step 개념이 도입된 RNN은 입력 길이 고정하지 않아도 된다. \n",
        "- RNNLM은 기본적으로 예측 과정에서 이전 시점의 출력을 현재 시점의 입력으로 한다.  \n",
        "-  훈련 과정   \n",
        "  - 이전 시점의 예측 결과를 다음 시점의 입력으로 넣으면서 예측하는 것 X   \n",
        "  -  what will the fat cat sit 시퀀스를 모델의 입력으로 넣으면, will the fat cat sit on를 예측하도록 훈련   \n",
        "  - **교사 강요(teacher forcing)** 기법\n",
        "    - 실제 알고 있는 정답을 t+1 시점의 입력으로 사용한다.   \n",
        "  - 출력층 activation function : softmax 함수   \n",
        "  - loss function : cross-entropy function   \n",
        "\n",
        "- RNNLM 구조   \n",
        "  ![](https://user-images.githubusercontent.com/90624848/217490462-e33d77a1-e8c8-45dd-8c29-dec1b2a7dab4.png)   \n",
        "  - input layer   \n",
        "    - 4번째 입력 단어인 fat의 원-핫 벡터가 입력\n",
        "  - output layer   \n",
        "    - 모델이 예측해야하는 정답인 cat의 원-핫 벡터는 출력층에서 예측한 값의 오차를 구하기 위해 사용   \n",
        "    - 이 오차로부터 손실 함수를 사용해 신경망이 학습   \n",
        "\n",
        "![](https://user-images.githubusercontent.com/90624848/217491085-d9e56e8b-ad22-4d90-9e43-9bacb81f5a9c.png)   \n",
        "- 입력 단어의 원-핫 벡터 $x_t$를 입력 받은 RNLLM은 embedding layer를 지남.   \n",
        "- embedding layer: $e_{t} = lookup(x_{t})$   \n",
        "- hidden layer : $h_{t} = tanh(W_{x} e_{t} + W_{h}h_{t−1} + b)$  \n",
        "  - 이전 시점의 은닉 상태인 $h_{t-1}$과 함께 다음 연산을 하여 현재 시점의 은닉 상태 $h_t$ 계산   \n",
        "- output layer : $\\hat{y_{t}} = softmax(W_{y}h_{t} + b)$\n",
        "  - V차원의 벡터는 소프트맥스 함수를 지나면서 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀐다.   \n",
        "  - $\\hat{y_{t}}$ : RNNLM의 t시점의 예측값\n"
      ],
      "metadata": {
        "id": "oFx7l7axRZ-A"
      }
    }
  ]
}